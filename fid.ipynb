{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cbe1fab-61b5-43f0-843c-172468219cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import PIL.Image as Image\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "from fid import calculate_statistics_for_given_paths, calculate_frechet_distance\n",
    "from fastai.vision.all import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e75b0864-4e68-4681-a8fd-75896743fb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inception import InceptionV3, InceptionV3_sehun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8dde170-a056-4418-ad0e-6cc4085ba361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8870\n",
      "70962\n"
     ]
    }
   ],
   "source": [
    "class png_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dirs, transforms = None):\n",
    "        '''\n",
    "        file : paths for images\n",
    "        Note that in inception.py the inputs are transformed [0, 1] to [-1, 1]\n",
    "        So, here, input images should be in [0, 1]\n",
    "        '''\n",
    "        self.dirs = dirs\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dirs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        dir = self.dirs[i]\n",
    "        img = Image.open(dir)#.convert('RGB') <- 이거 시간 나름 걸림\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        return img\n",
    "\n",
    "\n",
    "\n",
    "transformations = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean = [0.3831, 0.2659, 0.1896], std = [0.2915, 0.2107, 0.1708]),\n",
    "])\n",
    "\n",
    "valid_list = list(map(str, list(Path('/home/DB/SuGAr/RF/val').rglob('*png'))))\n",
    "print(len(valid_list))\n",
    "train_list = list(map(str, list(Path('/home/DB/SuGAr/RF/trn').rglob('*png'))))\n",
    "print(len(train_list))\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = png_dataset(dirs = train_list, transforms = transformations)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = 1024,\n",
    "    pin_memory = True,\n",
    "    num_workers = 4,\n",
    "    shuffle = False)\n",
    "\n",
    "valid_dataset = png_dataset(dirs = valid_list, transforms = transformations)\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    dataset = valid_dataset,\n",
    "    batch_size = 1024,\n",
    "    pin_memory = True,\n",
    "    num_workers = 4,\n",
    "    shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75ec8f13-a042-4eea-ae3c-145db1b73bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0126, -0.0016,  0.0909])\n",
      "calculating statistics for RF_ref_train\n",
      "total images :  70962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [02:50<00:00,  2.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70962, 2048)\n",
      "calculating statistics for RF_ref_valid\n",
      "total images :  8870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:26<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8870, 2048)\n",
      "saving statistics done for 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7177827708652273"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n",
    "model = InceptionV3([block_idx])\n",
    "\n",
    "for i in model.parameters():\n",
    "    pa = i\n",
    "    break\n",
    "print(pa[0,0,0])\n",
    "\n",
    "\n",
    "dataloader_dict = {\n",
    "    'RF_ref_train' : train_loader,\n",
    "    'RF_ref_valid' : valid_loader,\n",
    "}\n",
    "\n",
    "\n",
    "batch_size = 1024\n",
    "device = 'cuda'\n",
    "dims = 2048\n",
    "calculate_statistics_for_given_paths(\n",
    "    dataloader_dict, batch_size, device, dims)\n",
    "\n",
    "\n",
    "RF_rdf_val_m = np.load('./np_saves/RF_ref_valid_m.npy')\n",
    "RF_rdf_val_s = np.load('./np_saves/RF_ref_valid_s.npy')\n",
    "RF_rdf_train_m = np.load('./np_saves/RF_ref_train_m.npy')\n",
    "RF_rdf_train_s = np.load('./np_saves/RF_ref_train_s.npy')\n",
    "\n",
    "\n",
    "calculate_frechet_distance(\n",
    "    RF_rdf_val_m, RF_rdf_val_s, RF_rdf_train_m, RF_rdf_train_s, eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6368d527-89c9-47cc-b0f2-93bb265687a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2103, -0.3441, -0.0344])\n",
      "calculating statistics for sehun_RF_ref_train\n",
      "total images :  70962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 16/70 [00:40<01:20,  1.48s/it]"
     ]
    }
   ],
   "source": [
    "block_idx = InceptionV3_sehun.BLOCK_INDEX_BY_DIM[2048]\n",
    "model = InceptionV3_sehun([block_idx])\n",
    "\n",
    "for i in model.parameters():\n",
    "    pa = i\n",
    "    break\n",
    "print(pa[0,0,0])\n",
    "    \n",
    "dataloader_dict = {\n",
    "    'sehun_RF_ref_train' : train_loader,\n",
    "    'sehun_RF_ref_valid' : valid_loader,\n",
    "}\n",
    "\n",
    "\n",
    "batch_size = 1024\n",
    "device = 'cuda'\n",
    "dims = 2048\n",
    "calculate_statistics_for_given_paths(\n",
    "    dataloader_dict, batch_size, device, dims)\n",
    "\n",
    "\n",
    "RF_rdf_val_m = np.load('./np_saves/sehun_RF_ref_valid_m.npy')\n",
    "RF_rdf_val_s = np.load('./np_saves/sehun_RF_ref_valid_s.npy')\n",
    "RF_rdf_train_m = np.load('./np_saves/sehun_RF_ref_train_m.npy')\n",
    "RF_rdf_train_s = np.load('./np_saves/sehun_RF_ref_train_s.npy')\n",
    "\n",
    "\n",
    "calculate_frechet_distance(\n",
    "    RF_rdf_val_m, RF_rdf_val_s, RF_rdf_train_m, RF_rdf_train_s, eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8554d135-b0b0-485f-93a3-ed93c12955b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d561f93-c19e-4441-80b2-7e18a96d5da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de07510-4bec-474f-90f4-6b7cfcf4aab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a607859-2d65-43c9-976d-87d1102a5d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by sehun\n",
    "mu_val, std_val = np.load('../FID/mu_val.npy'), np.load('../FID/std_val.npy')\n",
    "mu_trn, std_trn = np.load('../FID/mu_trn.npy'), np.load('../FID/std_trn.npy')\n",
    "calculate_frechet_distance(mu_val, std_val, mu_trn, std_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ba29be-2a1a-4e82-956f-84a53ef05520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb98e1b-86f9-447c-945a-d45eb3773049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
